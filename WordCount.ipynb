{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2a51ee9-6fee-4043-8b9c-d7a0df8738ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import sys, os\n",
    "\n",
    "# Setting the root directory as a string.\n",
    "root = r'<ROOT LOCATION OF REPORT FOLDER>'\n",
    "\n",
    "# Combining the root directory with the target directory to create the full path\n",
    "path = os.path.join(root, \"targetdirectory\")\n",
    "\n",
    "# Create empty lists to store the full file paths and file names\n",
    "fullFilePaths = []\n",
    "fileNames = []\n",
    "\n",
    "# Walk through all the subdirectories and files within the root directory\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    # Loop through all the files in each subdirectory\n",
    "    for name in files:\n",
    "        # Create the full file path by joining the path and file name\n",
    "        fullFilePaths.append(os.path.join(path, name))\n",
    "\n",
    "# The final output will be the full file paths of all the files within the specified root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "066bc48c-1f4a-49c0-b05a-4fc443c1ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library to work with dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the filepath of the GRI excel file using a raw string to prevent backslash escaping\n",
    "griFullFileName = r'<GRI FILE LOCATION>'\n",
    "\n",
    "# Open the GRI excel file using pd.ExcelFile() and save it to the variable xls\n",
    "xls = pd.ExcelFile(griFullFileName)\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "fullGRI = {}\n",
    "\n",
    "# Loop through each sheet in the Excel file except for the \"Overview\" sheet\n",
    "# and read the sheet into a dataframe using pd.read_excel()\n",
    "# Add each dataframe to the fullGRI dictionary with the sheet name as the key\n",
    "for sheet_name in xls.sheet_names[1:21]:\n",
    "    fullGRI[sheet_name] = pd.read_excel(griFullFileName, sheet_name = sheet_name, header=1)\n",
    "\n",
    "# Read the \"Overview\" sheet into a dataframe separately and add it to the fullGRI dictionary with the key \"Overview\"\n",
    "fullGRI[\"Overview\"] = pd.read_excel(griFullFileName, sheet_name = \"Overview\")\n",
    "\n",
    "# Create a new dataframe named total_df by concatenating all dataframes in the fullGRI dictionary except for the \"Overview\" sheet\n",
    "# Set ignore_index argument to True to create a new index for the concatenated dataframe\n",
    "total_df = fullGRI[\"1999\"]\n",
    "for sheet_name in xls.sheet_names[2:21]:\n",
    "    total_df = pd.concat([total_df, fullGRI[sheet_name]], ignore_index=True)\n",
    "\n",
    "# Modify the 'Name' column in total_df by removing all spaces using the str.replace() method\n",
    "total_df['Name'] = total_df['Name'].str.replace(' ', '')\n",
    "\n",
    "# The resulting concatenated dataframe total_df contains all GRI data from 1999 to 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb416117-62fb-4508-a85a-bf64f1feb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expressions library to work with patterns\n",
    "import re\n",
    "\n",
    "# Create an empty dataframe with two columns named \"Organization\" and \"Year\"\n",
    "df = pd.DataFrame(columns=[\"Organization\", \"Year\"])\n",
    "\n",
    "# Loop through each file path in the fullFilePaths list\n",
    "for filePath in fullFilePaths:\n",
    "    # Extract the file name without extension from the file path\n",
    "    fileName = os.path.splitext(os.path.split(filePath)[1])[0]\n",
    "\n",
    "    # Create a list of two items by splitting the file name using \"_\" as a separator\n",
    "    # The first item is the organization name, and the second item is the year\n",
    "    row = [fileName.split(\"_\")[0], fileName.split(\"_\")[-1]]\n",
    "\n",
    "    # Add the row to the dataframe at the next available index using df.loc[]\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "# Convert the \"Year\" column to numeric values using pd.to_numeric()\n",
    "df['Year'] = pd.to_numeric(df['Year'])\n",
    "\n",
    "# The resulting dataframe df contains information about the PDF files, including the organization name and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "518444fa-bfbf-4c63-b968-d6e2172bf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: GRI DATAFRAME and PDF FILES DATAFRAME\n",
    "import numpy as np\n",
    "\n",
    "# merge the two dataframes based on 'Organization' and 'Year' columns\n",
    "almost_df = pd.merge(df, total_df,  how='left', left_on=['Organization','Year'], right_on = ['Name','Publication Year'])\n",
    "\n",
    "# drop the 'Name' and 'Publication Year' columns from the merged dataframe\n",
    "final_df = almost_df.drop(['Name','Publication Year'], axis=1)\n",
    "\n",
    "# drop duplicate rows based on 'Organization' and 'Year' columns\n",
    "final_df = final_df.drop_duplicates(subset=['Organization', 'Year']).reset_index(drop=True)\n",
    "\n",
    "# add a new column called \"WordCount\" with NaN values\n",
    "final_df[\"WordCount\"] = np.nan\n",
    "\n",
    "# OUTPUT: ONE FULL MERGED DATAFRAME WITH ALL THE PDF FILES AND GRI INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef83dca-97de-4c5c-ad39-49c98017fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stopwords from the NLTK corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create an empty set to store all stop words\n",
    "all_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a list of additional languages for which to include stopwords\n",
    "languages = [\"spanish\", \"chinese\", \"portuguese\", \"greek\", \"russian\", \n",
    "             \"italian\", \"finnish\", \"german\", \"indonesian\", \"norwegian\", \n",
    "             \"swedish\", \"french\", \"turkish\", \"hungarian\", \"romanian\"]\n",
    "\n",
    "# Loop through each language and add the corresponding stopwords to the set\n",
    "for language in languages:\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    all_stop_words = all_stop_words.union(stop_words)\n",
    "\n",
    "# The purpose of this code is to create a big list of stopwords for multiple languages. \n",
    "# The first step is to initialize an empty set with the stop words for the English language, as provided by the nltk.corpus module.\n",
    "# Next, a list of additional languages for which to include stopwords is defined. \n",
    "# A loop is then used to iterate over each language in the list, and for each language, the corresponding set of stopwords is obtained using the stopwords.words() function and stored in the 'stop_words' variable.\n",
    "# The union of the 'stop_words' set and the 'all_stop_words' set is then taken and stored in the 'all_stop_words' set, to create a bigger set of stopwords for multiple languages. \n",
    "# The final output is a big list of all the needed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3729e-866c-493d-8f47-8319d410a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "\n",
    "# Loop through each file path in the list\n",
    "index = 0\n",
    "for filePath in fullFilePaths:\n",
    "    # Print the current index after every 10 files are processed\n",
    "    if index % 10 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    try:\n",
    "        # Create a pdf reader object and an empty text variable\n",
    "        reader = PdfReader(filePath)\n",
    "        text = \"\"\n",
    "\n",
    "        # Extract the text from each page and append it to the text variable\n",
    "        for page_number in range(len(reader.pages)):\n",
    "            page = reader.pages[page_number]\n",
    "            page_text = page.extract_text().lower()\n",
    "            text += page_text\n",
    "\n",
    "        # Remove non-alphabetic characters and extra whitespace\n",
    "        clean_text = re.sub(r'[^a-zA-Z\\s]', '', text).replace('\\n', '')\n",
    "        clean_text = re.sub(r'\\s{2,}', ' ', clean_text).lower()\n",
    "\n",
    "        # Tokenize the text into individual words\n",
    "        tokens = word_tokenize(clean_text)\n",
    "\n",
    "        # Remove single-letter words and stopwords\n",
    "        clean_tokens = [i for i in tokens if len(i) > 1]\n",
    "        filtered_tokens = [word for word in clean_tokens if not word in all_stop_words]\n",
    "\n",
    "        # Lemmatize the words to their root form\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "        # Count the frequency of each word and store the 5000 most common words\n",
    "        counts = FreqDist(lemmatized_tokens)\n",
    "        token_counts = counts.most_common(5000)\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "        token_counts = \"An Error occured and no information was gathered\"\n",
    "    \n",
    "    # Add the token counts to the final dataframe\n",
    "    final_df.at[index, \"WordCount\"] = token_counts\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04c70d-1d96-4b9f-9211-195444e21560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file location to which the final dataframe will be exported\n",
    "file_location = r'<EXPORT FILE LOCATION>'\n",
    "\n",
    "# Export the final dataframe to the defined file location as a CSV file, without the index column\n",
    "final_df.to_csv(file_location, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
