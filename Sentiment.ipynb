{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9708b4b4-85ae-47bc-91a1-b06f4ec8c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import sys, os\n",
    "\n",
    "# Setting the root directory as a string.\n",
    "root = r'<ROOT LOCATION OF REPORT FOLDER>'\n",
    "\n",
    "# Combining the root directory with the target directory to create the full path\n",
    "path = os.path.join(root, \"targetdirectory\")\n",
    "\n",
    "# Create empty lists to store the full file paths and file names\n",
    "fullFilePaths = []\n",
    "fileNames = []\n",
    "\n",
    "# Walk through all the subdirectories and files within the root directory\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    # Loop through all the files in each subdirectory\n",
    "    for name in files:\n",
    "        # Create the full file path by joining the path and file name\n",
    "        fullFilePaths.append(os.path.join(path, name))\n",
    "\n",
    "# The final output will be the full file paths of all the files within the specified root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1b5a8a-669f-45cb-81ef-bd140ad97a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library to work with dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the filepath of the GRI excel file using a raw string to prevent backslash escaping\n",
    "griFullFileName = r'<GRI FILE LOCATION>'\n",
    "\n",
    "# Open the GRI excel file using pd.ExcelFile() and save it to the variable xls\n",
    "xls = pd.ExcelFile(griFullFileName)\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "fullGRI = {}\n",
    "\n",
    "# Loop through each sheet in the Excel file except for the \"Overview\" sheet\n",
    "# and read the sheet into a dataframe using pd.read_excel()\n",
    "# Add each dataframe to the fullGRI dictionary with the sheet name as the key\n",
    "for sheet_name in xls.sheet_names[1:21]:\n",
    "    fullGRI[sheet_name] = pd.read_excel(griFullFileName, sheet_name = sheet_name, header=1)\n",
    "\n",
    "# Read the \"Overview\" sheet into a dataframe separately and add it to the fullGRI dictionary with the key \"Overview\"\n",
    "fullGRI[\"Overview\"] = pd.read_excel(griFullFileName, sheet_name = \"Overview\")\n",
    "\n",
    "# Create a new dataframe named total_df by concatenating all dataframes in the fullGRI dictionary except for the \"Overview\" sheet\n",
    "# Set ignore_index argument to True to create a new index for the concatenated dataframe\n",
    "total_df = fullGRI[\"1999\"]\n",
    "for sheet_name in xls.sheet_names[2:21]:\n",
    "    total_df = pd.concat([total_df, fullGRI[sheet_name]], ignore_index=True)\n",
    "\n",
    "# Modify the 'Name' column in total_df by removing all spaces using the str.replace() method\n",
    "total_df['Name'] = total_df['Name'].str.replace(' ', '')\n",
    "\n",
    "# The resulting concatenated dataframe total_df contains all GRI data from 1999 to 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2f8882-e03b-437e-a3b8-ca86ab2b5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expressions library to work with patterns\n",
    "import re\n",
    "\n",
    "# Create an empty dataframe with two columns named \"Organization\" and \"Year\"\n",
    "df = pd.DataFrame(columns=[\"Organization\", \"Year\"])\n",
    "\n",
    "# Loop through each file path in the fullFilePaths list\n",
    "for filePath in fullFilePaths:\n",
    "    # Extract the file name without extension from the file path\n",
    "    fileName = os.path.splitext(os.path.split(filePath)[1])[0]\n",
    "\n",
    "    # Create a list of two items by splitting the file name using \"_\" as a separator\n",
    "    # The first item is the organization name, and the second item is the year\n",
    "    row = [fileName.split(\"_\")[0], fileName.split(\"_\")[-1]]\n",
    "\n",
    "    # Add the row to the dataframe at the next available index using df.loc[]\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "# Convert the \"Year\" column to numeric values using pd.to_numeric()\n",
    "df['Year'] = pd.to_numeric(df['Year'])\n",
    "\n",
    "# The resulting dataframe df contains information about the PDF files, including the organization name and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6f8600b-6150-42f0-b408-045db2b555df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Merge the two dataframes by left join using common columns 'Organization' and 'Year'\n",
    "almost_df = pd.merge(df, total_df,  how='left', left_on=['Organization','Year'], right_on = ['Name','Publication Year'])\n",
    "\n",
    "# Drop the columns 'Name' and 'Publication Year' from the merged dataframe\n",
    "final_df = almost_df.drop(['Name','Publication Year'], axis=1)\n",
    "\n",
    "# Remove duplicates in the columns 'Organization' and 'Year' and reset the index\n",
    "final_df = final_df.drop_duplicates(subset=['Organization', 'Year']).reset_index(drop=True)\n",
    "\n",
    "# Add two new columns 'Overall sentiment' and 'sentiment' to the final dataframe with initial values of NaN\n",
    "final_df[\"Overall sentiment\"] = np.nan\n",
    "final_df[\"sentiment\"] = np.nan\n",
    "\n",
    "# The purpose of the new columns is not clear from this code alone, but it is likely that they will be used to store sentiment analysis results based on the text in the PDF files. \n",
    "# The resulting dataframe 'final_df' is the fully merged dataframe with all the PDF files and GRI information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666bb20-7a62-4b85-beef-3aa6f9b11a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from PyPDF2 import PdfReader\n",
    "from langdetect import detect\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize an index variable to keep track of the current row in the dataframe\n",
    "index = 0\n",
    "\n",
    "# Loop through each file path in the list of full file paths\n",
    "for filePath in fullFilePaths:\n",
    "\n",
    "    # Print the current index every 10 files for progress monitoring\n",
    "    if index % 10 == 0:\n",
    "        print(index)\n",
    "\n",
    "    try:\n",
    "        # Create a PdfReader object and initialize an empty text variable\n",
    "        reader = PdfReader(filePath)\n",
    "        text = \"\"\n",
    "\n",
    "        # Extract text from a selection of pages (from 35% to 65% of the total pages)\n",
    "        first_page = round(len(reader.pages) * 0.35)\n",
    "        last_page = round(len(reader.pages) * 0.65)\n",
    "        for page_number in range(first_page, last_page):\n",
    "            page = reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text\n",
    "\n",
    "        # Analyze the sentiment of the extracted text using VADER sentiment analyzer\n",
    "        scores = analyzer.polarity_scores(text)\n",
    "\n",
    "        # Determine the overall sentiment based on the compound score\n",
    "        if scores['compound'] > 0:\n",
    "            OverallSentiment = \"Positive\"\n",
    "        elif scores['compound'] < 0:\n",
    "            OverallSentiment = \"Negative\"\n",
    "        else:\n",
    "            OverallSentiment = \"Neutral\"\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "\n",
    "    # Add the overall sentiment and sentiment scores to the corresponding columns in the dataframe\n",
    "    final_df.at[index, \"Overall sentiment\"] = OverallSentiment\n",
    "    final_df.at[index, \"sentiment\"] = scores\n",
    "\n",
    "    # Increment the index to move on to the next row in the dataframe\n",
    "    index += 1\n",
    "\n",
    "# The purpose of this loop is to analyze the sentiment of the text in each PDF file and add the results to the corresponding rows in the 'final_df' dataframe. \n",
    "# The sentiment analysis is performed using VADER (Valence Aware Dictionary and sEntiment Reasoner), which is a rule-based sentiment analysis tool specifically designed for social media text. \n",
    "# The extracted text is only from a selection of pages, between 35% and 65% of the total pages. \n",
    "# The overall sentiment is determined based on the compound score of the sentiment analysis. \n",
    "# If an exception occurs during the loop, the error message \"An exception occurred\" is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ca879e3-df09-4816-bd9b-d91d0e0ccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file location to which the final dataframe will be exported\n",
    "file_location = r'<EXPORT FILE LOCATION>'\n",
    "\n",
    "# Export the final dataframe to the defined file location as a CSV file, without the index column\n",
    "final_df.to_csv(file_location, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26affbd5-44ae-4824-a77f-b418277d7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file into a dataframe\n",
    "df2 = pd.read_csv(file_location)\n",
    "\n",
    "# Convert the 'sentiment' column from a string to a dictionary\n",
    "df2.sentiment = df2.sentiment.apply(literal_eval)\n",
    "\n",
    "# Normalize the nested 'sentiment' dictionary into separate columns\n",
    "df2 = df2.join(pd.json_normalize(df2.pop('sentiment')))\n",
    "\n",
    "# The purpose of this code is to convert the 'sentiment' column from a string to a dictionary, and then normalize the nested dictionary into separate columns. \n",
    "# The ast.literal_eval() function is used to safely evaluate the string expression in the 'sentiment' column as a Python literal (in this case, a dictionary). \n",
    "# The resulting 'sentiment' column is now a dictionary. \n",
    "# The pd.json_normalize() function is then used to convert the nested dictionary into separate columns in the dataframe. \n",
    "# The original 'sentiment' column is removed from the dataframe and replaced with the separate columns that were created from the normalized dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aba3b9ea-f3cb-4577-a619-11ee1ac631e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final dataframe to the defined file location as a CSV file, without the index column\n",
    "df2.to_csv(file_location, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
