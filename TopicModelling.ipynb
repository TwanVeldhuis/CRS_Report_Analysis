{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9708b4b4-85ae-47bc-91a1-b06f4ec8c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import sys, os\n",
    "\n",
    "# Setting the root directory as a string.\n",
    "root = r'<ROOT LOCATION OF REPORT FOLDER>'\n",
    "\n",
    "# Combining the root directory with the target directory to create the full path\n",
    "path = os.path.join(root, \"targetdirectory\")\n",
    "\n",
    "# Create empty lists to store the full file paths and file names\n",
    "fullFilePaths = []\n",
    "fileNames = []\n",
    "\n",
    "# Walk through all the subdirectories and files within the root directory\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    # Loop through all the files in each subdirectory\n",
    "    for name in files:\n",
    "        # Create the full file path by joining the path and file name\n",
    "        fullFilePaths.append(os.path.join(path, name))\n",
    "\n",
    "# The final output will be the full file paths of all the files within the specified root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1b5a8a-669f-45cb-81ef-bd140ad97a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library to work with dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the filepath of the GRI excel file using a raw string to prevent backslash escaping\n",
    "griFullFileName = r'<GRI FILE LOCATION>'\n",
    "\n",
    "# Open the GRI excel file using pd.ExcelFile() and save it to the variable xls\n",
    "xls = pd.ExcelFile(griFullFileName)\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "fullGRI = {}\n",
    "\n",
    "# Loop through each sheet in the Excel file except for the \"Overview\" sheet\n",
    "# and read the sheet into a dataframe using pd.read_excel()\n",
    "# Add each dataframe to the fullGRI dictionary with the sheet name as the key\n",
    "for sheet_name in xls.sheet_names[1:21]:\n",
    "    fullGRI[sheet_name] = pd.read_excel(griFullFileName, sheet_name = sheet_name, header=1)\n",
    "\n",
    "# Read the \"Overview\" sheet into a dataframe separately and add it to the fullGRI dictionary with the key \"Overview\"\n",
    "fullGRI[\"Overview\"] = pd.read_excel(griFullFileName, sheet_name = \"Overview\")\n",
    "\n",
    "# Create a new dataframe named total_df by concatenating all dataframes in the fullGRI dictionary except for the \"Overview\" sheet\n",
    "# Set ignore_index argument to True to create a new index for the concatenated dataframe\n",
    "total_df = fullGRI[\"1999\"]\n",
    "for sheet_name in xls.sheet_names[2:21]:\n",
    "    total_df = pd.concat([total_df, fullGRI[sheet_name]], ignore_index=True)\n",
    "\n",
    "# Modify the 'Name' column in total_df by removing all spaces using the str.replace() method\n",
    "total_df['Name'] = total_df['Name'].str.replace(' ', '')\n",
    "\n",
    "# The resulting concatenated dataframe total_df contains all GRI data from 1999 to 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e2f8882-e03b-437e-a3b8-ca86ab2b5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expressions library to work with patterns\n",
    "import re\n",
    "\n",
    "# Create an empty dataframe with two columns named \"Organization\" and \"Year\"\n",
    "df = pd.DataFrame(columns=[\"Organization\", \"Year\"])\n",
    "\n",
    "# Loop through each file path in the fullFilePaths list\n",
    "for filePath in fullFilePaths:\n",
    "    # Extract the file name without extension from the file path\n",
    "    fileName = os.path.splitext(os.path.split(filePath)[1])[0]\n",
    "\n",
    "    # Create a list of two items by splitting the file name using \"_\" as a separator\n",
    "    # The first item is the organization name, and the second item is the year\n",
    "    row = [fileName.split(\"_\")[0], fileName.split(\"_\")[-1]]\n",
    "\n",
    "    # Add the row to the dataframe at the next available index using df.loc[]\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "# Convert the \"Year\" column to numeric values using pd.to_numeric()\n",
    "df['Year'] = pd.to_numeric(df['Year'])\n",
    "\n",
    "# The resulting dataframe df contains information about the PDF files, including the organization name and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f8600b-6150-42f0-b408-045db2b555df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Merge the PDF files dataframe (df) with the GRI dataframe (total_df) using the organization name and year as the keys\n",
    "almost_df = pd.merge(df, total_df,  how='left', left_on=['Organization','Year'], right_on = ['Name','Publication Year'])\n",
    "\n",
    "# Drop the \"Name\" and \"Publication Year\" columns from the merged dataframe\n",
    "final_df = almost_df.drop(['Name','Publication Year'], axis=1)\n",
    "\n",
    "# Drop any duplicate rows based on the \"Organization\" and \"Year\" columns\n",
    "# This ensures that each row in the resulting dataframe corresponds to a unique organization and year combination\n",
    "final_df = final_df.drop_duplicates(subset=['Organization', 'Year']).reset_index(drop=True)\n",
    "\n",
    "# add a new column called \"Topics\" with NaN values\n",
    "final_df[\"Topics\"] = np.nan\n",
    "# OUTPUT: ONE FULL MERGED DATAFRAME WITH ALL THE PDF FILES AND GRI INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25ea9b8-39a1-4b3e-bfed-b8964ae58b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stopwords from the NLTK corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create an empty set to store all stop words\n",
    "all_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a list of additional languages for which to include stopwords\n",
    "languages = [\"spanish\", \"chinese\", \"portuguese\", \"greek\", \"russian\", \n",
    "             \"italian\", \"finnish\", \"german\", \"indonesian\", \"norwegian\", \n",
    "             \"swedish\", \"french\", \"turkish\", \"hungarian\", \"romanian\"]\n",
    "\n",
    "# Loop through each language and add the corresponding stopwords to the set\n",
    "for language in languages:\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    all_stop_words = all_stop_words.union(stop_words)\n",
    "\n",
    "# The purpose of this code is to create a big list of stopwords for multiple languages. \n",
    "# The first step is to initialize an empty set with the stop words for the English language, as provided by the nltk.corpus module.\n",
    "# Next, a list of additional languages for which to include stopwords is defined. \n",
    "# A loop is then used to iterate over each language in the list, and for each language, the corresponding set of stopwords is obtained using the stopwords.words() function and stored in the 'stop_words' variable.\n",
    "# The union of the 'stop_words' set and the 'all_stop_words' set is then taken and stored in the 'all_stop_words' set, to create a bigger set of stopwords for multiple languages. \n",
    "# The final output is a big list of all the needed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fc130-ac41-4486-a0cd-25c63a5f0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from gensim import corpora, models\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to get unique words from a list\n",
    "def getUniqueWords(allWords):\n",
    "    uniqueWords = [] \n",
    "    for i in allWords:\n",
    "        if not i in uniqueWords:\n",
    "            uniqueWords.append(i)\n",
    "    return uniqueWords\n",
    "\n",
    "# Initialize index counter for tracking progress\n",
    "index = 0\n",
    "\n",
    "# Iterate through each PDF file path in the list of file paths\n",
    "for filePath in fullFilePaths:\n",
    "    # Print progress update every 10 files\n",
    "    if index % 10 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    try:\n",
    "        # Read PDF file using PyPDF2 and extract text\n",
    "        reader = PdfReader(filePath)\n",
    "        text = \"\"\n",
    "        first_page = round(len(reader.pages) * 0.10)\n",
    "        last_page = round(len(reader.pages) * 0.90)\n",
    "\n",
    "        for page_number in range(len(reader.pages)):\n",
    "            page = reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text\n",
    "\n",
    "        # Tokenize words\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and punctuation\n",
    "        clean_tokens = [token.lower() for token in tokens if token.lower() not in all_stop_words and token.isalpha()]\n",
    "\n",
    "        # Lemmatize tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "\n",
    "        # Find bigrams\n",
    "        bigram_finder = BigramCollocationFinder.from_words(lemmatized_tokens)\n",
    "        bigrams = bigram_finder.nbest(BigramAssocMeasures().raw_freq, 10)\n",
    "\n",
    "        # Create bag of words model\n",
    "        dictionary = corpora.Dictionary([lemmatized_tokens])\n",
    "        corpus = [dictionary.doc2bow(lemmatized_tokens)]\n",
    "\n",
    "        # Train LDA model\n",
    "        lda_model = models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "        \n",
    "        # Print topics\n",
    "        lst = []\n",
    "        for i, topic in lda_model.show_topics(num_topics=10, formatted=False):\n",
    "            lst += [word[0] for word in topic]\n",
    "        \n",
    "        # Get an unique list of all the top topics\n",
    "        topics = getUniqueWords(lst)\n",
    "            \n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "        topics = \"error\"\n",
    "    \n",
    "    # Add the topics to the final data frame for this PDF file\n",
    "    final_df.at[index, \"Topics\"] = topics\n",
    "    \n",
    "    # Increment the index counter\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca879e3-df09-4816-bd9b-d91d0e0ccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file location to which the final dataframe will be exported\n",
    "file_location = r'<EXPORT FILE LOCATION>'\n",
    "\n",
    "# Export the final dataframe to the defined file location as a CSV file, without the index column\n",
    "final_df.to_csv(file_location, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
